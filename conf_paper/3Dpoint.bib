@article{Ma2019,
abstract = {Three-dimensional human body models are widely used in the analysis of human pose and motion. Existing models, however, are learned from minimally-clothed 3D scans and thus do not generalize to the complexity of dressed people in common images and videos. Additionally, current models lack the expressive power needed to represent the complex non-linear geometry of pose-dependent clothing shapes. To address this, we learn a generative 3D mesh model of clothed people from 3D scans with varying pose and clothing. Specifically, we train a conditional Mesh-VAE-GAN to learn the clothing deformation from the SMPL body model, making clothing an additional term in SMPL. Our model is conditioned on both pose and clothing type, giving the ability to draw samples of clothing to dress different body shapes in a variety of styles and poses. To preserve wrinkle detail, our Mesh-VAE-GAN extends patchwise discriminators to 3D meshes. Our model, named CAPE, represents global shape and fine local structure, effectively extending the SMPL body model to clothing. To our knowledge, this is the first generative model that directly dresses 3D human body meshes and generalizes to different poses. The model, code and data are available for research purposes at https://cape.is.tue.mpg.de.},
archivePrefix = {arXiv},
arxivId = {1907.13615},
author = {Ma, Qianli and Yang, Jinlong and Ranjan, Anurag and Pujades, Sergi and Pons-Moll, Gerard and Tang, Siyu and Black, Michael J},
eprint = {1907.13615},
file = {::},
pages = {6469--6478},
title = {{Learning to Dress 3D People in Generative Clothing}},
url = {https://cape.is.tue.mpg.de. http://arxiv.org/abs/1907.13615},
year = {2019}
}
@article{Samad2016,
abstract = {{\textcopyright} 2015 Elsevier Ltd.Autism Spectrum Disorders (ASD) can impair non-verbal communication including the variety and extent of facial expressions in social and interpersonal communication. These impairments may appear as differential traits in the physiology of facial muscles of an individual with ASD when compared to a typically developing individual. The differential traits in the facial expressions as shown by facial muscle-specific changes (also known as'facial oddity' for subjects with ASD) may be measured visually. However, this mode of measurement may not discern the subtlety in facial oddity distinctive to ASD. Earlier studies have used intrusive electrophysiological sensors on the facial skin to gauge facial muscle actions from quantitative physiological data. This study demonstrates, for the first time in the literature, novel quantitative measures for facial oddity recognition using non-intrusive facial imaging sensors such as video and 3D optical cameras. An Institutional Review Board (IRB) approved that pilot study has been conducted on a group of individuals consisting of eight participants with ASD and eight typically developing participants in a control group to capture their facial images in response to visual stimuli. The proposed computational techniques and statistical analyses reveal higher mean of actions in the facial muscles of the ASD group versus the control group. The facial muscle-specific evaluation reveals intense yet asymmetric facial responses as facial oddity in participants with ASD. This finding about the facial oddity may objectively define measurable differential markers in the facial expressions of individuals with ASD.},
author = {Samad, M.D. and Bobzien, J.L. and Harrington, J.W. and Iftekharuddin, K.M.},
doi = {10.1016/j.optlastec.2015.09.030},
file = {::},
issn = {00303992},
journal = {Optics and Laser Technology},
keywords = {Autism Spectrum Disorder,Facial expressions,Facial oddity,Optical imaging},
pages = {221--228},
title = {{Non-intrusive optical imaging of face to probe physiological traits in Autism Spectrum Disorder}},
volume = {77},
year = {2016}
}
@article{Samad2016a,
abstract = {{\^{A}}{\textcopyright} 2013 IEEE.The state-of-the-art methods in classifying 3-D representation of the face involve challenges in extracting representative features directly from the large volume of facial data. These methods mostly ignore the effect of pose distortions on 3-D facial data and entail heavy computations as well as manual processing steps. This work proposes a novel Frenet frame-based generalized space curve representation method for 3-D pose-invariant face and facial expression recognition and classification. Three-dimensional facial curves are extracted from either frontal or synthetically posed 3-D facial data to derive the proposed Frenet frame-based features. A mathematical framework shows the proof of pose invariance property for the features. The effectiveness of the proposed method is evaluated in two recognition tasks: 3-D face recognition (3D-FR) and 3-D facial expression recognition (3D-FER) using benchmarked 3-D datasets. The proposed framework yields 96{\%} rank-I recognition rate for 3D-FR and 91.4{\%} area under ROC curves for six basic 3D-FER. The performance evaluation also shows that the proposed mathematical framework yields pose-invariant 3D-FR and 3D-FER for a wide range of pose angles. This pose invariance property of the Frenet frame-based features alleviates the need for an expensive 3-D face registration in the preprocessing step, which, in turn, enables a faster processing time. The evaluation results further suggest that the proposed method is not only computationally efficient and versatile, but also offers competitive performance when compared with the existing state-of-the-art methods reported for either 3D-FR or 3D-FER.},
author = {Samad, M.D. and Iftekharuddin, K.M.},
doi = {10.1109/THMS.2016.2515602},
file = {:Users/mdsamad/Dropbox/RESEARCH/PAPERS{\_}THESES/My Papers/THMS.pdf:pdf},
issn = {21682291},
journal = {IEEE Transactions on Human-Machine Systems},
keywords = {3-D face classification,Facial curves,Frenet frame,facial expression,pose-invariant recognition},
number = {4},
title = {{Frenet Frame-Based Generalized Space Curve Representation for Pose-Invariant Classification and Recognition of 3-D Face}},
volume = {46},
year = {2016}
}
@inproceedings{Zhang2019,
abstract = {To alleviate the cost of collecting and annotating large-scale '3D object' point cloud data, we propose an unsupervised learning approach to learn features from an unlabeled point cloud dataset by using part contrasting and object clustering with deep graph convolutional neural networks (GCNNs). In the contrast learning step, all the samples in the 3D object dataset are cut into two parts and put into a 'part' dataset. Then a contrast learning GCNN (ContrastNet) is trained to verify whether two randomly sampled parts from the part dataset belong to the same object. In the cluster learning step, the trained ContrastNet is applied to all the samples in the original 3D object dataset to extract features, which are used to group the samples into clusters. Then another GCNN for clustering learning (ClusterNet) is trained from the orignal 3D data to predict the cluster IDs of all the training samples. The contrasting learning forces the ContrastNet to learn semantic features of objects, while the ClusterNet improves the quality of learned features by being trained to discover objects that belong to the same semantic categories by using cluster IDs. We have conducted extensive experiments to evaluate the proposed framework on point cloud classification tasks. The proposed unsupervised learning approach obtains comparable performance to the state-of-the-art with heavier shape auto-encoding unsupervised feature extraction methods. We have also tested the networks on object recognition using partial 3D data, by simulating occlusions and perspective views, and obtained practically useful results. The code of this work is publicly available at: Https://github.com/lingzhang1/ContrastNet.},
author = {Zhang, Ling and Zhu, Zhigang},
booktitle = {Proceedings - 2019 International Conference on 3D Vision, 3DV 2019},
doi = {10.1109/3DV.2019.00051},
file = {::},
isbn = {9781728131313},
keywords = {Classification,Deep Learning,Graph convolutional neural network,Point Cloud,Unsupervised Learning},
month = {sep},
pages = {395--404},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Unsupervised Feature Learning for Point Cloud Understanding by Contrasting and Clustering Using Graph Convolutional Neural Networks}},
year = {2019}
}
@article{Yan2020,
abstract = {This paper presents a system for online learning of human classifiers by mobile service robots using 3D LiDAR sensors, and its experimental evaluation in a large indoor public space. The learning framework requires a minimal set of labelled samples (e.g. one or several samples) to initialise a classifier. The classifier is then retrained iteratively during operation of the robot. New training samples are generated automatically using multi-target tracking and a pair of “experts” to estimate false negatives and false positives. Both classification and tracking utilise an efficient real-time clustering algorithm for segmentation of 3D point cloud data. We also introduce a new feature to improve human classification in sparse, long-range point clouds. We provide an extensive evaluation of our the framework using a 3D LiDAR dataset of people moving in a large indoor public space, which is made available to the research community. The experiments demonstrate the influence of the system components and improved classification of humans compared to the state-of-the-art.},
author = {Yan, Zhi and Duckett, Tom and Bellotto, Nicola},
doi = {10.1007/s10514-019-09883-y},
file = {::},
issn = {15737527},
journal = {Autonomous Robots},
keywords = {3D LiDAR-based tracking,Dataset,Human detection,Online learning,Point cloud segmentation},
month = {jan},
number = {2},
pages = {147--164},
publisher = {Springer},
title = {{Online learning for 3D LiDAR-based human detection: experimental analysis of point cloud clustering and classification methods}},
url = {https://link.springer.com/article/10.1007/s10514-019-09883-y},
volume = {44},
year = {2020}
}
@article{Li2017,
abstract = {The recognition process of 3D face in cloud environment is vulnerable to the interference of external environment, resulting in poor recognition accuracy, therefore, a 3D face recognition method in cloud environment based on semi supervised clustering algorithm is proposed in this paper, after Harris feature points of successful matching of a 2D image are mapped into 3D space, surface fitting method estimated by the least squares is used to extract the curvature information corresponding to feature points, maximum and minimum principal curvatures are constructed as the final curvature eigenvector. The semi supervised clustering algorithm is introduced to perform the cluster judgement to decide if the sample is labeled 3D face sample. According to the class probability or membership degree of 3D face samples to preselect partial samples, labeled 3D face samples training classifier is established, after the second-selection is processed for the preselected 3D face samples according to the classification confidence, the selected results and the predicted labels are added into labeled samples. The iterations are applied to the semi supervised clustering algorithm to cluster the original labeled samples, new labeled samples and label the rest of the unlabeled samples,. The iterative process is repeated until all samples have been marked, and the labeled samples are identified 3D face data. The experiment combined proposed method with cloud computing platform, the experimental results show that the proposed method has high recognition accuracy and efficiency.},
author = {Li, Cuixia and Tan, Yingjun and Wang, Dingbiao and Ma, Peijie},
doi = {10.1007/s11042-016-3670-1},
file = {:Users/mdsamad/Library/Application Support/Mendeley Desktop/Downloaded/Li et al. - 2017 - Research on 3D face recognition method in cloud environment based on semi supervised clustering algorithm.pdf:pdf},
issn = {15737721},
journal = {Multimedia Tools and Applications},
keywords = {3D,Cloud environment,Face,Recognition,Semi supervised clustering},
month = {aug},
number = {16},
pages = {17055--17073},
publisher = {Springer New York LLC},
title = {{Research on 3D face recognition method in cloud environment based on semi supervised clustering algorithm}},
url = {https://link.springer.com/article/10.1007/s11042-016-3670-1},
volume = {76},
year = {2017}
}
@article{Cao2014,
abstract = {We present FaceWarehouse, a database of 3D facial expressions for visual computing applications. We use Kinect, an off-the-shelf RGBD camera, to capture 150 individuals aged 7-80 from various ethnic backgrounds. For each person, we captured the RGBD data of her different expressions, including the neutral expression and 19 other expressions such as mouth-opening, smile, kiss, etc. For every RGBD raw data record, a set of facial feature points on the color image such as eye corners, mouth contour, and the nose tip are automatically localized, and manually adjusted if better accuracy is required. We then deform a template facial mesh to fit the depth data as closely as possible while matching the feature points on the color image to their corresponding points on the mesh. Starting from these fitted face meshes, we construct a set of individual-specific expression blendshapes for each person. These meshes with consistent topology are assembled as a rank-3 tensor to build a bilinear face model with two attributes: identity and expression. Compared with previous 3D facial databases, for every person in our database, there is a much richer matching collection of expressions, enabling depiction of most human facial actions. We demonstrate the potential of FaceWarehouse for visual computing with four applications: facial image manipulation, face component transfer, real-time performance-based facial image animation, and facial animation retargeting from video to image. {\textcopyright} 2014 IEEE.},
author = {Cao, Chen and Weng, Yanlin and Zhou, Shun and Tong, Yiying and Zhou, Kun},
doi = {10.1109/TVCG.2013.249},
file = {::},
issn = {10772626},
journal = {IEEE Transactions on Visualization and Computer Graphics},
keywords = {Face modeling,RGBD camera,face database,facial animation,mesh deformation},
number = {3},
pages = {413--425},
publisher = {IEEE Computer Society},
title = {{FaceWarehouse: A 3D facial expression database for visual computing}},
volume = {20},
year = {2014}
}
@inproceedings{Saito2016,
abstract = {We introduce the concept of unconstrained real-time 3Dfacial performance capture through explicit semantic segmentation in the RGB input. To ensure robustness, cutting edge supervised learning approaches rely on large training datasets of face images captured in the wild. While impressive tracking quality has been demonstrated for faces that are largely visible, any occlusion due to hair, accessories, or hand-to-face gestures would result in significant visual artifacts and loss of tracking accuracy. The modeling of occlusions has been mostly avoided due to its immense space of appearance variability. To address this curse of high dimensionality, we perform tracking in unconstrained images assuming non-face regions can be fully masked out. Along with recent breakthroughs in deep learning, we demonstrate that pixel-level facial segmentation is possible in real-time by repurposing convolutional neural networks designed originally for general semantic segmentation. We develop an efficient architecture based on a two-stream deconvolution network with complementary characteristics, and introduce carefully designed training samples and data augmentation strategies for improved segmentation accuracy and robustness. We adopt a state-of-the-art regression-based facial tracking framework with segmented face images as training, and demonstrate accurate and uninterrupted facial performance capture in the presence of extreme occlusion and even side views. Furthermore, the resulting segmentation can be directly used to composite partial 3D face models on the input images and enable seamless facial manipulation tasks, such as virtual make-up or face replacement.},
archivePrefix = {arXiv},
arxivId = {1604.02647},
author = {Saito, Shunsuke and Li, Tianye and Li, Hao},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-46484-8_15},
eprint = {1604.02647},
file = {:Users/mdsamad/Library/Application Support/Mendeley Desktop/Downloaded/Saito, Li, Li - 2016 - Real-time facial segmentation and performance capture from RGB input.pdf:pdf},
isbn = {9783319464831},
issn = {16113349},
keywords = {Deep convolutional neural network,Face segmentation,Real-time facial performance capture,Regression},
pages = {244--261},
publisher = {Springer Verlag},
title = {{Real-time facial segmentation and performance capture from RGB input}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-46484-8{\_}15},
volume = {9912 LNCS},
year = {2016}
}
@inproceedings{Nirkin2018,
abstract = {We show that even when face images are unconstrained and arbitrarily paired, face swapping between them is quite simple. To this end, we make the following contributions. (a) Instead of tailoring systems for face segmentation, as others previously proposed, we show that a standard fully convolutional network (FCN) can achieve remarkably fast and accurate segmentations, provided that it is trained on a rich enough example set. For this purpose, we describe novel data collection and generation routines which provide challenging segmented face examples. (b) We use our segmentations for robust face swapping under unprecedented conditions. (c) Unlike previous work, our swapping is robust enough to allow for extensive quantitative tests. To this end, we use the Labeled Faces in the Wild (LFW) benchmark and measure the effect of intra- and inter-subject face swapping on recognition. We show that our intra-subject swapped faces remain as recognizable as their sources, testifying to the effectiveness of our method. In line with established perceptual studies, we show that better face swapping produces less recognizable inter-subject results. This is the first time this effect was quantitatively demonstrated by machine vision systems.},
author = {Nirkin, Yuval and Masi, Iacopo and Tuǎn, Anh Trǎn and Hassner, Tal and Medioni, G{\'{e}}rard},
booktitle = {Proceedings - 13th IEEE International Conference on Automatic Face and Gesture Recognition, FG 2018},
doi = {10.1109/FG.2018.00024},
file = {:Users/mdsamad/Library/Application Support/Mendeley Desktop/Downloaded/Nirkin et al. - 2018 - On face segmentation, face swapping, and face perception.pdf:pdf},
isbn = {9781538623350},
keywords = {Face recognition,Face segmentation,Face swapping},
month = {jun},
pages = {98--105},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{On face segmentation, face swapping, and face perception}},
year = {2018}
}
@techreport{Uy2019,
abstract = {Deep learning techniques for point cloud data have demonstrated great potentials in solving classical problems in 3D computer vision such as 3D object classification and segmentation. Several recent 3D object classification methods have reported state-of-the-art performance on CAD model datasets such as ModelNet40 with high accuracy (∼92{\%}). Despite such impressive results, in this paper, we argue that object classification is still a challenging task when objects are framed with real-world settings. To prove this, we introduce ScanObjectNN, a new real-world point cloud object dataset based on scanned indoor scene data. From our comprehensive benchmark, we show that our dataset poses great challenges to existing point cloud classification techniques as objects from real-world scans are often cluttered with background and/or are partial due to occlusions. We identify three key open problems for point cloud object classification, and propose new point cloud classification neural networks that achieve state-of-the-art performance on classifying objects with cluttered background. Our dataset and code are publicly available in our project page 1 .},
author = {Uy, Mikaela Angelina and Pham, Quang-Hieu and Hua, Binh-Son and Nguyen, Duc Thanh and Yeung, Sai-Kit},
file = {:Users/mdsamad/Library/Application Support/Mendeley Desktop/Downloaded/Uy et al. - 2019 - Revisiting Point Cloud Classification A New Benchmark Dataset and Classification Model on Real-World Data.pdf:pdf},
pages = {1588--1597},
title = {{Revisiting Point Cloud Classification: A New Benchmark Dataset and Classification Model on Real-World Data}},
url = {https://hkust-vgd.github.io/scanobjectnn/},
year = {2019}
}
@techreport{Cheraghian2020,
abstract = {Zero-shot learning, the task of learning to recognize new classes not seen during training, has received considerable attention in the case of 2D image classification. However despite the increasing ubiquity of 3D sensors, the corresponding 3D point cloud classification problem has not been meaningfully explored and introduces new challenges. This paper extends, for the first time, transductive Zero-Shot Learning (ZSL) and Generalized Zero-Shot Learning (GZSL) approaches to the domain of 3D point cloud classification. To this end, a novel triplet loss is developed that takes advantage of unlabeled test data. While designed for the task of 3D point cloud classification, the method is also shown to be applicable to the more common use-case of 2D image classification. An extensive set of experiments is carried out, establishing state-of-the-art for ZSL and GZSL in the 3D point cloud domain, as well as demonstrating the applicability of the approach to the image domain. 1},
author = {Cheraghian, Ali and Rahman, Shafin and Campbell, Dylan and Petersson, Lars},
file = {:Users/mdsamad/Library/Application Support/Mendeley Desktop/Downloaded/Cheraghian et al. - 2020 - Transductive Zero-Shot Learning for 3D Point Cloud Classification.pdf:pdf},
pages = {923--933},
title = {{Transductive Zero-Shot Learning for 3D Point Cloud Classification}},
url = {https://github.},
year = {2020}
}
@inproceedings{Weinmann2017,
abstract = {In this paper, we focus on the automatic interpretation of 3D point cloud data in terms of associating a class label to each 3D point. While much effort has recently been spent on this research topic, little attention has been paid to the influencing factors that affect the quality of the derived classification results. For this reason, we investigate fundamental influencing factors making geometric features more or less relevant with respect to the classification task. We present a framework which consists of five components addressing point sampling, neighborhood recovery, feature extraction, classification and feature relevance assessment. To analyze the impact of the main influencing factors which are represented by the given point sampling and the selected neighborhood type, we present the results derived with different configurations of our framework for a commonly used benchmark dataset for which a reference labeling with respect to three structural classes (linear structures, planar structures and volumetric structures) as well as a reference labeling with respect to five semantic classes (Wire, Pole/Trunk, Fa{\c{c}}ade, Ground and Vegetation) is available.},
author = {Weinmann, M. and Jutzi, B. and Mallet, C. and Weinmann, M.},
booktitle = {ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences},
doi = {10.5194/isprs-annals-IV-1-W1-157-2017},
file = {::},
issn = {21949050},
keywords = {3D,Classification,Feature Extraction,Feature Relevance Assessment,Point Cloud},
month = {may},
number = {1W1},
pages = {157--164},
publisher = {Copernicus GmbH},
title = {{GEOMETRIC FEATURES and THEIR RELEVANCE for 3D POINT CLOUD CLASSIFICATION}},
volume = {4},
year = {2017}
}
@inproceedings{Zhang2018,
abstract = {Graph convolutional neural networks (Graph-CNNs) extend traditional CNNs to handle data that is supported on a graph. Major challenges when working with data on graphs are that the support set (the vertices of the graph) do not typically have a natural ordering, and in general, the topology of the graph is not regular (i.e., vertices do not all have the same number of neighbors). Thus, Graph-CNNs have huge potential to deal with 3D point cloud data which has been obtained from sampling a manifold. In this paper we develop a Graph-CNN for classifying 3D point cloud data, called PointGCN1. The architecture combines localized graph convolutions with two types of graph downsampling operations (also known as pooling). By the effective exploration of the point cloud local structure using the Graph-CNN, the proposed architecture achieves competitive performance on the 3D object classification benchmark ModelNet, and our architecture is more stable than competing schemes.},
archivePrefix = {arXiv},
arxivId = {1812.01711},
author = {Zhang, Yingxue and Rabbat, Michael},
booktitle = {ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings},
doi = {10.1109/ICASSP.2018.8462291},
eprint = {1812.01711},
file = {:Users/mdsamad/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Rabbat - 2018 - A Graph-CNN for 3D Point Cloud Classification.pdf:pdf},
isbn = {9781538646588},
issn = {15206149},
keywords = {3D point cloud data,Graph convolutional neural networks,Graph signal processing,Supervised learning},
month = {sep},
pages = {6279--6283},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{A Graph-CNN for 3D Point Cloud Classification}},
volume = {2018-April},
year = {2018}
}
@article{Hackel2017,
abstract = {This paper presents a new 3D point cloud classification benchmark data set with over four billion manually labelled points, meant as input for data-hungry (deep) learning methods. We also discuss first submissions to the benchmark that use deep convolutional neural networks (CNNs) as a work horse, which already show remarkable performance improvements over state-of-the-art. CNNs have become the de-facto standard for many tasks in computer vision and machine learning like semantic segmentation or object detection in images, but have no yet led to a true breakthrough for 3D point cloud labelling tasks due to lack of training data. With the massive data set presented in this paper, we aim at closing this data gap to help unleash the full potential of deep learning methods for 3D labelling tasks. Our semantic3D.net data set consists of dense point clouds acquired with static terrestrial laser scanners. It contains 8 semantic classes and covers a wide range of urban outdoor scenes: churches, streets, railroad tracks, squares, villages, soccer fields and castles. We describe our labelling interface and show that our data set provides more dense and complete point clouds with much higher overall number of labelled points compared to those already available to the research community. We further provide baseline method descriptions and comparison between methods submitted to our online system. We hope semantic3D.net will pave the way for deep learning methods in 3D point cloud labelling to learn richer, more general 3D representations, and first submissions after only a few months indicate that this might indeed be the case.},
archivePrefix = {arXiv},
arxivId = {1704.03847},
author = {Hackel, Timo and Savinov, Nikolay and Ladicky, Lubor and Wegner, Jan D. and Schindler, Konrad and Pollefeys, Marc},
eprint = {1704.03847},
file = {:Users/mdsamad/Library/Application Support/Mendeley Desktop/Downloaded/Hackel et al. - 2017 - Semantic3D.net A new Large-scale Point Cloud Classification Benchmark.pdf:pdf},
month = {apr},
title = {{Semantic3D.net: A new Large-scale Point Cloud Classification Benchmark}},
url = {http://arxiv.org/abs/1704.03847},
year = {2017}
}
@article{Ben-Shabat2018,
abstract = {Modern robotic systems are often equipped with a direct three-dimensional (3-D) data acquisition device, e.g., LiDAR, which provides a rich 3-D point cloud representation of the surroundings. This representation is commonly used for obstacle avoidance and mapping. Here, we propose a new approach for using point clouds for another critical robotic capability, semantic understanding of the environment (i.e., object classification). Convolutional neural networks (CNNs), that perform extremely well for object classification in 2-D images, are not easily extendible to 3-D point clouds analysis. It is not straightforward due to point clouds' irregular format and a varying number of points. The common solution of transforming the point cloud data into a 3-D voxel grid needs to address severe accuracy versus memory size tradeoffs. In this letter, we propose a novel, intuitively interpretable, 3-D point cloud representation called 3-D modified Fisher vectors. Our representation is hybrid as it combines a coarse discrete grid structure with continuous generalized Fisher vectors. Using the grid enables us to design a new CNN architecture for real-time point cloud classification. In a series of performance analysis experiments, we demonstrate competitive results or even better than state of the art on challenging benchmark datasets while maintaining robustness to various data corruptions.},
author = {Ben-Shabat, Yizhak and Lindenbaum, Michael and Fischer, Anath},
doi = {10.1109/LRA.2018.2850061},
file = {::},
issn = {23773766},
journal = {IEEE Robotics and Automation Letters},
keywords = {Deep learning in robotics and automation,computer vision for other robotic applications,computer vision for transportation,recognition},
month = {oct},
number = {4},
pages = {3145--3152},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{3DmFV: Three-dimensional point cloud classification in real-time using convolutional neural networks}},
volume = {3},
year = {2018}
}
